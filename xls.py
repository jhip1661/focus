import os
import json
import datetime
import random
import logging
import re
from typing import List, Tuple
import difflib

import gspread
from oauth2client.service_account import ServiceAccountCredentials
from openai import OpenAI  # âœ… v1.x í´ë¼ì´ì–¸íŠ¸ ì‚¬ìš©

# â”€â”€ ë¡œê¹… ì„¤ì • â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
logging.basicConfig(level=logging.INFO,
                    format="%(asctime)s [%(levelname)s] %(message)s")

# â”€â”€ ì„œë¹„ìŠ¤ ê³„ì • JSON ë¡œë“œ & ê²€ì¦ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
RAW_JSON = os.getenv("GSHEET_CREDENTIALS_JSON")
if not RAW_JSON:
    raise ValueError("âŒ í™˜ê²½ë³€ìˆ˜ 'GSHEET_CREDENTIALS_JSON'ì´ ëˆ„ë½ë˜ì—ˆìŠµë‹ˆë‹¤.")
try:
    creds_info = json.loads(RAW_JSON)
    logging.info("âœ… JSON íŒŒì‹±(ì›ë³¸) ì„±ê³µ")
except json.JSONDecodeError:
    fixed = RAW_JSON.replace('\\n', '\n')
    try:
        creds_info = json.loads(fixed)
        logging.info("âœ… JSON íŒŒì‹±(ë³µì›) ì„±ê³µ")
    except json.JSONDecodeError as e:
        raise ValueError(f"âŒ SERVICE_ACCOUNT_JSON íŒŒì‹± ì‹¤íŒ¨: {e}")

if "private_key" not in creds_info or not creds_info["private_key"].startswith(
        "-----BEGIN PRIVATE KEY-----"):
    raise ValueError("âŒ ì˜ëª»ëœ ì„œë¹„ìŠ¤ ê³„ì • JSONì…ë‹ˆë‹¤. í™˜ê²½ë³€ìˆ˜ì— ì „ì²´ JSONì„ ì •í™•íˆ ë³µì‚¬í–ˆëŠ”ì§€ í™•ì¸í•˜ê³ , "
                     "ì„œë¹„ìŠ¤ ê³„ì • ì´ë©”ì¼ì´ ìŠ¤í”„ë ˆë“œì‹œíŠ¸ì— ê³µìœ ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”.")

SCOPES = [
    "https://spreadsheets.google.com/feeds",
    "https://www.googleapis.com/auth/drive"
]
try:
    gs_creds = ServiceAccountCredentials.from_json_keyfile_dict(
        creds_info, SCOPES)
    logging.info("âœ… Google Sheets ì¸ì¦ ê°ì²´ ìƒì„± ì™„ë£Œ")
except Exception as e:
    raise RuntimeError(f"âŒ ì¸ì¦ ì •ë³´ ë¡œë“œ ì‹¤íŒ¨: {e}")

# â”€â”€ OpenAI ì„¤ì • â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
if not OPENAI_API_KEY:
    raise ValueError("âŒ í™˜ê²½ë³€ìˆ˜ 'OPENAI_API_KEY'ê°€ ì—†ìŠµë‹ˆë‹¤.")
client = OpenAI(api_key=OPENAI_API_KEY)

# â”€â”€ ìƒìˆ˜ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
SOURCE_DB_ID = os.getenv("SOURCE_DB_ID")
TARGET_DB_ID = os.getenv("TARGET_DB_ID")
SIMILARITY_THRESHOLD = 0.6
MAX_RETRIES = 5
SELECT_COUNT = 5


def init_worksheet(sheet_id: str, sheet_name: str, header: List[str] = None):
    """ì›Œí¬ì‹œíŠ¸ ë¡œë“œ í˜¹ì€ ìƒì„±, í•„ìš”ì‹œ í—¤ë” ì¶”ê°€"""
    gs_client = gspread.authorize(gs_creds)
    try:
        ws = gs_client.open_by_key(sheet_id).worksheet(sheet_name)
    except gspread.exceptions.WorksheetNotFound:
        ws = gs_client.open_by_key(sheet_id).add_worksheet(title=sheet_name,
                                                           rows="1000",
                                                           cols="20")
    if header:
        current = ws.get_all_values()
        if not current or all(cell == '' for cell in current[0]):
            ws.clear()
            ws.append_row(header)
    return ws


def calculate_similarity(text1: str, text2: str) -> float:
    return difflib.SequenceMatcher(None, text1, text2).ratio()


def clean_content(text: str) -> str:
    return re.sub(r'(?m)^(ì„œë¡ |ë¬¸ì œ ìƒí™©|ì‹¤ë¬´ íŒ|ê²°ë¡ )[:\-]?\s*', '', text).strip()


def build_messages_from_prompt(prompt_config: List[str], title: str,
                               content: str) -> List[dict]:
    purpose, tone, para, emphasis, fmt, etc = prompt_config
    system_msg = f"{purpose}\n\n{tone}\n\n{para}\n\n{emphasis}\n\n{fmt}\n\n{etc}"
    user_msg = f"ë‹¤ìŒ ê¸€ì„ ì¤‘ë³µë˜ì§€ ì•Šë„ë¡ ì¬ì‘ì„±í•´ì¤˜:\n\nì œëª©: {title}\në‚´ìš©: {content}"
    return [{
        "role": "system",
        "content": system_msg
    }, {
        "role": "user",
        "content": user_msg
    }]


def regenerate_unique_post(original_title: str, original: str,
                           existing_texts: List[str],
                           prompt_config: List[str]) -> Tuple[str, float, int]:
    regen, score = original, 1.0
    for i in range(1, MAX_RETRIES + 1):
        messages = build_messages_from_prompt(prompt_config, original_title,
                                              original)
        etc_lower = prompt_config[-1].lower()
        if "2500ì" in etc_lower:
            max_tokens = 2500
        elif "2000ì" in etc_lower:
            max_tokens = 2000
        else:
            max_tokens = 3000

        resp = client.chat.completions.create(model="gpt-3.5-turbo",
                                              messages=messages,
                                              temperature=0.8,
                                              max_tokens=max_tokens)
        candidate = clean_content(resp.choices[0].message.content.strip())
        sim = max((calculate_similarity(candidate, t) for t in existing_texts),
                  default=0)
        if sim < SIMILARITY_THRESHOLD:
            return candidate, sim, i
        regen, score = candidate, sim

    return regen, score, MAX_RETRIES


def regenerate_title(content: str) -> str:
    system = "ë„ˆëŠ” ë§ˆì¼€íŒ… ì½˜í…ì¸  ì „ë¬¸ê°€ì•¼. ì•„ë˜ ë‚´ìš©ì„ ë³´ê³  í´ë¦­ì„ ìœ ë„í•˜ëŠ” ì§§ì€ ì œëª©ì„ ì‘ì„±í•´ì¤˜."
    resp = client.chat.completions.create(model="gpt-3.5-turbo",
                                          messages=[{
                                              "role": "system",
                                              "content": system
                                          }, {
                                              "role": "user",
                                              "content": content[:1000]
                                          }],
                                          temperature=0.7,
                                          max_tokens=800)
    title = resp.choices[0].message.content.strip()
    return re.sub(r'^.*?:\s*', '', title)


def extract_tags(text: str) -> List[str]:
    prompt = f"ë‹¤ìŒ ê¸€ì—ì„œ ì‹¤ë¬´ ì¤‘ì‹¬ ëª…ì‚¬ 5ê°œë¥¼ í•´ì‹œíƒœê·¸(#í‚¤ì›Œë“œ) í˜•íƒœë¡œ ì¶”ì¶œí•´ì¤˜. ê¸€: {text}"
    resp = client.chat.completions.create(model="gpt-3.5-turbo",
                                          messages=[{
                                              "role":
                                              "system",
                                              "content":
                                              "ë‹¹ì‹ ì€ íƒœê·¸ ì¶”ì¶œ ì „ë¬¸ê°€ì…ë‹ˆë‹¤."
                                          }, {
                                              "role": "user",
                                              "content": prompt
                                          }],
                                          temperature=0,
                                          max_tokens=50)
    return re.findall(r'#(\w+)', resp.choices[0].message.content)[:5]


def translate_text(text: str, lang: str) -> str:
    langs = {
        "English": "English",
        "Chinese": "Simplified Chinese",
        "Japanese": "Japanese"
    }
    target = langs.get(lang, lang)
    resp = client.chat.completions.create(model="gpt-3.5-turbo",
                                          messages=[{
                                              "role":
                                              "system",
                                              "content":
                                              f"ë‹¤ìŒì„ {target}ë¡œ ë²ˆì—­í•´ì¤˜."
                                          }, {
                                              "role": "user",
                                              "content": text
                                          }],
                                          temperature=0.5,
                                          max_tokens=2000)
    return resp.choices[0].message.content.strip()


def find_matching_image(tags: List[str], image_ws) -> str:
    for row in image_ws.get_all_values()[1:]:
        if any(tag in row[0] for tag in tags):
            return row[1]
    return ""


def now_str() -> str:
    return datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S")


def extract_valid_prompt(prompt_ws) -> List[List[str]]:
    return [
        r[4:10] for r in prompt_ws.get_all_values()[1:]
        if r[1].strip() == 'ì¬ìƒì‚°' and r[3].strip() == 'Y'
    ]


def pick_rows(src_ws, count=SELECT_COUNT) -> List[List[str]]:
    rows = src_ws.get_all_values()[1:]
    return random.sample(rows, min(count, len(rows))) if rows else []


def estimate_cost(tokens: int, model: str = "gpt-3.5-turbo") -> float:
    rate = 0.0015 if model == "gpt-3.5-turbo" else 0.03
    return round(tokens / 1000 * rate, 4)


def process_regeneration():
    logging.info("ğŸ“Œ process_regeneration() ì‹œì‘")

    src_ws = init_worksheet(SOURCE_DB_ID, "xls")
    prompt_ws = init_worksheet(SOURCE_DB_ID, "prompt")
    image_ws = init_worksheet(SOURCE_DB_ID, "image")
    info_ws = init_worksheet(
        TARGET_DB_ID, "information",
        ["ì‘ì„±ì¼ì‹œ", "ì œëª©", "ë‚´ìš©", "íƒœê·¸", "ì˜ë¬¸", "ì¤‘ë¬¸", "ì¼ë¬¸", "í‘œì ˆë¥ ", "ì´ë¯¸ì§€url"])

    selected = pick_rows(src_ws)
    logging.info(f"ğŸ¯ ì„ íƒëœ í–‰ ìˆ˜: {len(selected)}")
    if not selected:
        logging.warning("âš ï¸ ë³¸ë¬¸ ì‹œíŠ¸ì—ì„œ ì„ íƒí•  ìˆ˜ ìˆëŠ” í–‰ì´ ì—†ìŠµë‹ˆë‹¤.")
        return 0

    prompts = extract_valid_prompt(prompt_ws)
    logging.info(f"ğŸ¯ í”„ë¡¬í”„íŠ¸ ìˆ˜: {len(prompts)}")
    if not prompts:
        logging.warning("âš ï¸ ì‚¬ìš© ê°€ëŠ¥í•œ í”„ë¡¬í”„íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return 0

    config = random.choice(prompts)
    all_texts = [r[2] for r in src_ws.get_all_values()[1:] if len(r) > 2]
    total_tokens = 0

    for row in selected:
        title0 = row[1] if len(row) > 1 else ""
        orig = row[2] if len(row) > 2 else ""
        if not orig:
            logging.warning(f"âš ï¸ ë³¸ë¬¸ì´ ë¹„ì–´ ìˆìŒ: {row}")
            continue

        content, score, tries = regenerate_unique_post(title0, orig, all_texts,
                                                       config)
        total_tokens += tries * 3000
        new_title = regenerate_title(content)
        tags = extract_tags(content)
        en = translate_text(content, "English")
        zh = translate_text(content, "Chinese")
        ja = translate_text(content, "Japanese")
        img = find_matching_image(tags, image_ws)

        try:
            info_ws.append_row([
                now_str(), new_title, content, ", ".join(tags), en, zh, ja,
                f"{score:.2f}", img
            ])
            logging.info(
                f"âœ… '{new_title}' ì €ì¥ ì™„ë£Œ | ìœ ì‚¬ë„: {score:.2f} | ì¬ì‹œë„: {tries}íšŒ")
        except Exception as e:
            logging.error(f"âŒ ì‹œíŠ¸ ì“°ê¸° ì‹¤íŒ¨: {e}")

    logging.info(f"ğŸ’° ì˜ˆìƒ ë¹„ìš©: ${estimate_cost(total_tokens)}")
    return len(selected)


if __name__ == "__main__":
    process_regeneration()
