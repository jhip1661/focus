import os
import json
import datetime
import random
import logging
import re
import difflib
from typing import List, Tuple

import gspread
from google.oauth2.service_account import Credentials as GCredentials
import openai  # ëª¨ë“ˆ ì „ì²´ import

# â”€â”€ ì„œë¹„ìŠ¤ ê³„ì • JSON: env var ìš°ì„ , ì—†ìœ¼ë©´ ë¡œì»¬ íŒŒì¼ì—ì„œ ë¡œë“œ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
service_json = os.getenv("GSHEET_CREDENTIALS_JSON", "")
if service_json:
    try:
        creds_info = json.loads(service_json)
    except json.JSONDecodeError as e:
        raise ValueError(f"âŒ SERVICE_ACCOUNT_JSON íŒŒì‹± ì‹¤íŒ¨: {e}")
else:
    json_path = os.path.join(os.path.dirname(__file__), "focus-2025-458906-311d04096c93.json")
    if not os.path.exists(json_path):
        raise ValueError("âŒ í™˜ê²½ë³€ìˆ˜ 'GSHEET_CREDENTIALS_JSON'ì´ ì—†ê³ , ë¡œì»¬ JSON íŒŒì¼ë„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    with open(json_path, "r", encoding="utf-8") as f:
        creds_info = json.load(f)

if "private_key" not in creds_info or not creds_info["private_key"].startswith("-----BEGIN PRIVATE KEY-----"):
    raise ValueError("âŒ ì˜ëª»ëœ ì„œë¹„ìŠ¤ ê³„ì • JSONì…ë‹ˆë‹¤.")

# â–¶ Google Sheets/Drive ì¸ì¦ ê°ì²´ ìƒì„±
SCOPES = [
    "https://spreadsheets.google.com/feeds",
    "https://www.googleapis.com/auth/drive"
]
gs_creds = GCredentials.from_service_account_info(creds_info, scopes=SCOPES)

# âœ… OpenAI API í‚¤ ì„¤ì •
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
if not OPENAI_API_KEY:
    OPENAI_API_KEY = os.getenv("OPENAI_KEY") or os.getenv("OPENAI_SECRET")
if not OPENAI_API_KEY:
    env_path = os.path.join(os.path.dirname(__file__), ".env")
    if os.path.exists(env_path):
        with open(env_path, encoding="utf-8") as f:
            for line in f:
                if line.strip().startswith("OPENAI_API_KEY"):
                    key = line.strip().split("=", 1)[1].strip()
                    if key:
                        OPENAI_API_KEY = key
                    break
if not OPENAI_API_KEY:
    raise ValueError("âŒ í™˜ê²½ë³€ìˆ˜ 'OPENAI_API_KEY'ê°€ ëˆ„ë½ë˜ì—ˆìŠµë‹ˆë‹¤.")
openai.api_key = OPENAI_API_KEY
client = openai

# â”€â”€ ì„¤ì •ì •ë³´ì‹œíŠ¸ ë¡œë“œ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
CONFIG_SHEET_ID = "1h8FcZcDPFsCsdnHaLhDvr8WI2mQpd9raU0YLZ7KW8_8"
config_sheet = gspread.authorize(gs_creds).open_by_key(CONFIG_SHEET_ID).sheet1
config = config_sheet.get_all_records()[0]
input_db_url = config.get("ì…ë ¥ DB ì£¼ì†Œ")
posting_db_url = config.get("í¬ìŠ¤íŒ… DB ì£¼ì†Œ")
SOURCE_DB_ID = re.search(r"/d/([a-zA-Z0-9-_]+)", input_db_url).group(1) if input_db_url else None
TARGET_DB_ID = re.search(r"/d/([a-zA-Z0-9-_]+)", posting_db_url).group(1) if posting_db_url else None

# âœ… ë²ˆì—­ìš© GPT ëª¨ë¸: ê°€ì¥ ì €ë ´í•œ ëª¨ë¸ë¡œ í•˜ë“œì½”ë”©
TRANSLATION_MODEL = "gpt-3.5-turbo"

SIMILARITY_THRESHOLD = 0.5
MAX_RETRIES = 5

def init_worksheet(sheet_id: str, sheet_name: str, header: List[str] = None):
    ws = gspread.authorize(gs_creds).open_by_key(sheet_id).worksheet(sheet_name)
    if header:
        first = ws.get_all_values()[:1]
        if not first or all(cell == "" for cell in first[0]):
            ws.clear()
            ws.append_row(header)
    return ws

def calculate_similarity(a: str, b: str) -> float:
    return difflib.SequenceMatcher(None, a, b).ratio()

def clean_content(text: str) -> str:
    return re.sub(r'(?m)^(ì„œë¡ |ë¬¸ì œ ìƒí™©|ì‹¤ë¬´ íŒ|ê²°ë¡ )[:\-]?\s*', '', text).strip()

def build_messages_from_prompt(cfg: List[str], title: str, content: str) -> List[dict]:
    purpose, tone, para, emphasis, fmt, etc = cfg
    system = f"{purpose}\n\n{tone}\n\n{para}\n\n{emphasis}\n\n{fmt}\n\n{etc}"
    user = f"ë‹¤ìŒ ê¸€ì„ ì¤‘ë³µë˜ì§€ ì•Šë„ë¡ ì¬ì‘ì„±í•´ì¤˜:\n\nì œëª©: {title}\në‚´ìš©: {content}"
    return [
        {"role": "system", "content": system.strip()},
        {"role": "user",   "content": user.strip()},
    ]

def regenerate_unique_post(original_title: str, original: str, existing_texts: List[str], prompt_cfg: List[str], model_name: str) -> Tuple[str, float, int]:
    regen, score = original, 1.0
    for i in range(1, MAX_RETRIES + 1):
        msgs = build_messages_from_prompt(prompt_cfg, original_title, original)
        etc_lower = prompt_cfg[-1].lower()
        max_tokens = 3000
        if '2500ì' in etc_lower:
            max_tokens = 2500
        elif '2000ì' in etc_lower:
            max_tokens = 2000
        try:
            resp = client.ChatCompletion.create(
                model=model_name,
                messages=msgs,
                temperature=0.8,
                max_tokens=max_tokens,
            )
        except Exception as e:
            logging.warning(f"âš ï¸ GPT ìš”ì²­ ì‹¤íŒ¨ (ì‹œë„ {i}): {e}")
            continue

        candidate = clean_content(resp.choices[0].message.content or '')
        sim = max((calculate_similarity(candidate, ex) for ex in existing_texts), default=0)
        if sim < SIMILARITY_THRESHOLD:
            return candidate, sim, i
        regen, score = candidate, sim
    return regen, score, MAX_RETRIES

def regenerate_title(content: str) -> str:
    resp = client.ChatCompletion.create(
        model="gpt-4-turbo",
        messages=[
            {"role": "system", "content": "ë„ˆëŠ” ë§ˆì¼€íŒ… ì½˜í…ì¸  ì „ë¬¸ê°€ì•¼. ì§§ì€ ì œëª©ì„ ì‘ì„±í•´ì¤˜."},
            {"role": "user", "content": content[:1000]},
        ],
        temperature=0.7,
        max_tokens=800,
    )
    return re.sub(r'^.*?:\s*', '', resp.choices[0].message.content.strip())

def translate_text(text: str, lang: str) -> str:
    # âœ… í•­ìƒ gpt-3.5-turbo ì‚¬ìš©í•˜ë„ë¡ í•˜ë“œì½”ë”©
    resp = client.ChatCompletion.create(
        model=TRANSLATION_MODEL,
        messages=[
            {"role": "system", "content": f"ë‹¤ìŒì„ {lang}ë¡œ ë²ˆì—­í•´ì¤˜."},
            {"role": "user",   "content": text},
        ],
        temperature=0.5,
        max_tokens=2000,
    )
    return resp.choices[0].message.content.strip()

def now_str() -> str:
    return datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S")

def process_regeneration():
    logging.basicConfig(level=logging.INFO)
    logging.info("ğŸ“Œ process_regeneration() ì‹œì‘")

    src_ws = init_worksheet(SOURCE_DB_ID, "í™ë³´ì‹œíŠ¸")
    prompt_ws = init_worksheet(SOURCE_DB_ID, "í”„ë¡¬í”„íŠ¸ì‹œíŠ¸")
    image_ws = init_worksheet(SOURCE_DB_ID, "ì´ë¯¸ì§€ ì‹œíŠ¸")
    info_ws = init_worksheet(TARGET_DB_ID, "í™ë³´ì‹œíŠ¸")

    rows = src_ws.get_all_values()[1:]
    today = datetime.datetime.now().date()
    valid_rows = []
    for r in rows:
        norm = re.sub(r'[\.\s]+', '-', r[1].strip())
        norm = re.sub(r'-+', '-', norm)
        try:
            dl = datetime.datetime.strptime(norm, "%Y-%m-%d").date()
        except ValueError:
            continue
        if dl >= today:
            valid_rows.append(r)
    if not valid_rows:
        logging.warning("âš ï¸ ìœ íš¨í•œ ë§ˆì¼€íŒ… ì½˜í…ì¸ ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return 0

    total = 0
    prompt_header = prompt_ws.row_values(1)
    col_map = {name: idx for idx, name in enumerate(prompt_header)}
    run_idx = col_map.get("run_count", len(prompt_header))

    prompts = prompt_ws.get_all_values()[1:]
    for i, cfg in enumerate(prompts, start=2):
        if len(cfg) <= run_idx:
            continue
        if (
            cfg[col_map["ì¶œì²˜"]].strip() != "í™ë³´ì‹œíŠ¸" or
            cfg[col_map["í˜„ì¬ì‚¬ìš©ì—¬ë¶€"]].strip().upper() != "Y"
        ):
            continue
        category = cfg[col_map["êµ¬ë¶„íƒœê·¸"]].strip()
        if not category:
            continue

        item = random.choice(valid_rows)
        # âœ… ê¸°ì¡´ ê¸€ ë¦¬ìŠ¤íŠ¸ì—ì„œ ìê¸° ìì‹  ì œì™¸ (ì¤‘ë³µ ìƒì„± ë°©ì§€)
        existing_texts = [r[4] for r in valid_rows if r != item]

        prompt_fields = [
            "ì‘ì„±ì ì—­í•  ì„¤ëª…", "ì „ì²´ ì‘ì„± ì¡°ê±´", "ê¸€ êµ¬ì„±ë°©ì‹",
            "í•„ìˆ˜ í¬í•¨ í•­ëª©", "ë§ˆë¬´ë¦¬ ë¬¸ì¥", "ì¶”ê°€ ì§€ì‹œì‚¬í•­"
        ]
        prompt_cfg = [cfg[col_map[f]] for f in prompt_fields]

        orig_title = item[0]
        orig_cont = item[4]

        prev_count = int(cfg[run_idx]) if cfg[run_idx].isdigit() else 0
        interval = int(cfg[col_map["ê¸€ ê°„ê²©"]]) if cfg[col_map["ê¸€ ê°„ê²©"]].isdigit() else 1
        basic_mod = cfg[col_map["ê¸°ë³¸ gpt"]].strip() or "gpt-3.5-turbo"
        adv_mod = cfg[col_map["ê³ ê¸‰ gpt"]].strip() or basic_mod
        use_model = basic_mod if prev_count < interval else adv_mod
        new_count = prev_count + 1 if prev_count < interval else 0

        content, score, _ = regenerate_unique_post(
            orig_title, orig_cont, existing_texts, prompt_cfg, use_model
        )
        title = regenerate_title(content)

        image_tag = cfg[col_map["ì´ë¯¸ì§€íƒœê·¸"]].strip()
        img_header = image_ws.row_values(1)
        img_col_map = {name: idx for idx, name in enumerate(img_header)}
        d_idx = img_col_map.get("ì´ë¯¸ì§€íƒœê·¸")
        c_idx = img_col_map.get("ì´ë¯¸ì§€url")
        img = ""
        if d_idx is not None and c_idx is not None and image_tag:
            candidates = [
                row[c_idx].strip() for row in image_ws.get_all_values()[1:]
                if len(row) > d_idx and row[d_idx].strip() == image_tag
                   and len(row) > c_idx and row[c_idx].strip()
            ]
            if candidates:
                img = random.choice(candidates)

        en = translate_text(content, 'English')
        zh = translate_text(content, 'Chinese')
        ja = translate_text(content, 'Japanese')

        info_ws.append_row([
            now_str(), title, content, category, en, zh, ja, f"{score:.2f}", img
        ])

        prompt_ws.update_cell(i, run_idx + 1, str(new_count))
        total += 1

    logging.info(f"ğŸ’° ì´ ì €ì¥ëœ ê¸€ ìˆ˜: {total}")
    return total

if __name__ == "__main__":
    process_regeneration()
