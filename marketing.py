import os
import json
import datetime
import random
import logging
import re
import difflib
import requests       # ‚îÄ‚îÄ Ï∂îÍ∞Ä: HTTP ÏöîÏ≤≠Ïö© (ÌîΩÏÇ¨Î≤†Ïù¥, Ìè∞Ìä∏ Îã§Ïö¥Î°úÎìú)
import io             # ‚îÄ‚îÄ Ï∂îÍ∞Ä: Î∞îÏù¥Ìä∏ IO Ï≤òÎ¶¨
import textwrap       # ‚îÄ‚îÄ Ï∂îÍ∞Ä: ÌÖçÏä§Ìä∏ Ï§ÑÎ∞îÍøà
from PIL import Image, ImageDraw, ImageFont  # ‚îÄ‚îÄ Ï∂îÍ∞Ä: Pillow
import gspread
from google.oauth2.service_account import Credentials as GCredentials
from googleapiclient.discovery import build  # ‚îÄ‚îÄ Ï∂îÍ∞Ä: Drive API
from googleapiclient.http import MediaIoBaseUpload  # ‚îÄ‚îÄ Ï∂îÍ∞Ä: Drive ÏóÖÎ°úÎìú
import openai  # Î™®Îìà Ï†ÑÏ≤¥ import
from typing import List, Tuple

# ‚îÄ‚îÄ ÏÑúÎπÑÏä§ Í≥ÑÏ†ï JSON: env var Ïö∞ÏÑ†, ÏóÜÏúºÎ©¥ Î°úÏª¨ JSON ÌååÏùºÏóêÏÑú Î°úÎìú ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
service_json = os.getenv("GSHEET_CREDENTIALS_JSON", "")
if service_json:
    try:
        creds_info = json.loads(service_json)
    except json.JSONDecodeError as e:
        raise ValueError(f"‚ùå SERVICE_ACCOUNT_JSON ÌååÏã± Ïã§Ìå®: {e}")
else:
    json_path = os.path.join(os.path.dirname(__file__), "focus-2025-458906-311d04096c93.json")
    if not os.path.exists(json_path):
        raise ValueError("‚ùå ÌôòÍ≤ΩÎ≥ÄÏàò 'GSHEET_CREDENTIALS_JSON'Ïù¥ ÏóÜÍ≥†, Î°úÏª¨ JSON ÌååÏùºÎèÑ Ï∞æÏùÑ Ïàò ÏóÜÏäµÎãàÎã§.")
    with open(json_path, "r", encoding="utf-8") as f:
        creds_info = json.load(f)
if "private_key" not in creds_info or not creds_info["private_key"].startswith("-----BEGIN PRIVATE KEY-----"):
    raise ValueError("‚ùå ÏûòÎ™ªÎêú ÏÑúÎπÑÏä§ Í≥ÑÏ†ï JSONÏûÖÎãàÎã§.")

# ‚ñ∂ Google Sheets/Drive Ïù∏Ï¶ù Í∞ùÏ≤¥ ÏÉùÏÑ±
SCOPES = [
    "https://spreadsheets.google.com/feeds",
    "https://www.googleapis.com/auth/drive"
]
gs_creds = GCredentials.from_service_account_info(creds_info, scopes=SCOPES)
drive_service = build("drive", "v3", credentials=gs_creds)  # ‚îÄ‚îÄ Ï∂îÍ∞Ä: Drive API ÌÅ¥ÎùºÏù¥Ïñ∏Ìä∏

# ‚úÖ OpenAI API ÌÇ§ ÏÑ§Ï†ï
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY") or os.getenv("OPENAI_KEY") or os.getenv("OPENAI_SECRET")
if not OPENAI_API_KEY:
    env_path = os.path.join(os.path.dirname(__file__), ".env")
    if os.path.exists(env_path):
        with open(env_path, encoding="utf-8") as f:
            for line in f:
                if line.strip().startswith("OPENAI_API_KEY"):
                    OPENAI_API_KEY = line.strip().split("=", 1)[1].strip().strip('"')
                    break
if not OPENAI_API_KEY:
    raise ValueError("‚ùå ÌôòÍ≤ΩÎ≥ÄÏàò 'OPENAI_API_KEY'Í∞Ä ÎàÑÎùΩÎêòÏóàÏäµÎãàÎã§.")
openai.api_key = OPENAI_API_KEY
client = openai

# ‚îÄ‚îÄ ÏÑ§Ï†ïÏ†ïÎ≥¥ÏãúÌä∏ Î°úÎìú ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
CONFIG_SHEET_ID = "1h8FcZcDPFsCsdnHaLhDvr8WI2mQpd9raU0YLZ7KW8_8"
config_sheet = gspread.authorize(gs_creds).open_by_key(CONFIG_SHEET_ID).sheet1
config = config_sheet.get_all_records()[0]
input_db_url = config.get("ÏûÖÎ†• DB Ï£ºÏÜå")
posting_db_url = config.get("Ìè¨Ïä§ÌåÖ DB Ï£ºÏÜå")
SOURCE_DB_ID = re.search(r"/d/([a-zA-Z0-9-_]+)", input_db_url).group(1) if input_db_url else None
TARGET_DB_ID = re.search(r"/d/([a-zA-Z0-9-_]+)", posting_db_url).group(1) if posting_db_url else None

# ‚úÖ Î≤àÏó≠Ïö© GPT Î™®Îç∏: Í∞ÄÏû• Ï†ÄÎ†¥Ìïú Î™®Îç∏Î°ú ÌïòÎìúÏΩîÎî©
TRANSLATION_MODEL = "gpt-3.5-turbo"
SIMILARITY_THRESHOLD = 0.5
MAX_RETRIES = 5

# ‚îÄ‚îÄ PIXABAY_API_KEY: Íµ¨Í∏Ä ÏãúÌä∏ D2ÏóêÏÑú Î°úÎìú ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
PIXABAY_SHEET_URL = (
    "https://docs.google.com/spreadsheets/d/"
    "1-cTWVnP3sTwI_gcpuIKt1O-z1qAC8h9INp0/edit?gid=594125532"
)
pixabay_sh = gspread.authorize(gs_creds).open_by_url(PIXABAY_SHEET_URL)
pixabay_ws = pixabay_sh.get_worksheet(0)
PIXABAY_API_KEY = pixabay_ws.acell("D2").value.strip()  # ‚îÄ‚îÄ ÏàòÏ†ï: D2 ÏÖÄÏóêÏÑú API ÌÇ§ ÏùΩÍ∏∞

# ‚îÄ‚îÄ Î¨¥Î£å ÌïúÍ∏Ä Ìè∞Ìä∏(Noto Sans KR) ÏûêÎèô Îã§Ïö¥Î°úÎìú ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
FONT_URL = (
    "https://github.com/google/fonts/raw/main/ofl/notosanskr/"
    "NotoSansKR-Regular.ttf"
)
FONT_PATH = os.path.join(os.path.dirname(__file__), "NotoSansKR-Regular.ttf")
if not os.path.exists(FONT_PATH):
    resp = requests.get(FONT_URL)
    with open(FONT_PATH, "wb") as f:
        f.write(resp.content)  # ‚îÄ‚îÄ Ï∂îÍ∞Ä: Î¨¥Î£å Ìè∞Ìä∏ Îã§Ïö¥Î°úÎìú

# ‚îÄ‚îÄ Drive ÏóÖÎ°úÎìúÏö© Ìè¥Îçî ID ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
DRIVE_FOLDER_ID = "1-nj2HKlAjOu1C0R_0I7fyI83U4Y-gTZ8"


def init_worksheet(sheet_id: str, sheet_name: str, header: List[str] = None):
    ws = gspread.authorize(gs_creds).open_by_key(sheet_id).worksheet(sheet_name)
    if header:
        first = ws.get_all_values()[:1]
        if not first or all(cell == "" for cell in first[0]):
            ws.append_row(header)
    return ws


def calculate_similarity(a: str, b: str) -> float:
    return difflib.SequenceMatcher(None, a, b).ratio()


def clean_content(text: str) -> str:
    return re.sub(r'(?m)^(ÏÑúÎ°†|Î¨∏Ï†ú ÏÉÅÌô©|Ïã§Î¨¥ ÌåÅ|Í≤∞Î°†)[:\-]?\s*', '', text).strip()


def build_messages_from_prompt(
    cfg: List[str], title: str, content: str
) -> List[dict]:
    purpose, tone, para, emphasis, fmt, etc = cfg
    system = f"{purpose}\n\n{tone}\n\n{para}\n\n{emphasis}\n\n{fmt}\n\n{etc}"
    user = (
        f"Îã§Ïùå Í∏ÄÏùÑ Ï§ëÎ≥µÎêòÏßÄ ÏïäÎèÑÎ°ù Ïû¨ÏûëÏÑ±Ìï¥Ï§ò:\n\n"
        f"Ï†úÎ™©: {title}\nÎÇ¥Ïö©: {content}"
    )
    return [
        {"role": "system", "content": system.strip()},
        {"role": "user", "content": user.strip()},
    ]


def regenerate_unique_post(
    original_title: str,
    original: str,
    existing_texts: List[str],
    prompt_cfg: List[str],
    model_name: str
) -> Tuple[str, float, int]:
    regen, score = original, 1.0
    threshold = 0.6
    max_tokens = 3000
    for i in range(1, MAX_RETRIES + 1):
        msgs = build_messages_from_prompt(
            prompt_cfg, original_title, original
        )
        etc_lower = prompt_cfg[-1].lower()
        if '2500Ïûê' in etc_lower:
            max_tokens = 2500
        elif '2000Ïûê' in etc_lower:
            max_tokens = 2000
        try:
            resp = client.ChatCompletion.create(
                model=model_name,
                messages=msgs,
                temperature=0.8,
                max_tokens=max_tokens,
            )
        except Exception as e:
            logging.warning(
                f"‚ö†Ô∏è GPT ÏöîÏ≤≠ Ïã§Ìå® (ÏãúÎèÑ {i}): {e}"
            )
            continue
        candidate = clean_content(
            resp.choices[0].message.content or ''
        )
        sim = max(
            calculate_similarity(candidate, ex)
            for ex in existing_texts
        )
        if sim < threshold:
            return candidate, sim, i
        regen, score = candidate, sim
    logging.info(
        f"üîÅ MAX_RETRIES ÎèÑÎã¨ - ÌëúÏ†àÎ•† Í∏∞Ï§ÄÏùÑ 0.7Î°ú ÏôÑÌôîÌïòÏó¨ Ïû¨ÏãúÎèÑ: {original_title}"
    )
    for i in range(1, MAX_RETRIES + 1):
        try:
            resp = client.ChatCompletion.create(
                model=model_name,
                messages=msgs,
                temperature=0.8,
                max_tokens=max_tokens,
            )
        except Exception as e:
            logging.warning(
                f"‚ö†Ô∏è GPT ÏµúÏ¢Ö ÏöîÏ≤≠ Ïã§Ìå® (ÌëúÏ†àÎ•† 70% Í∏∞Ï§Ä, ÏãúÎèÑ {i}): {e}"
            )
            continue
        candidate = clean_content(
            resp.choices[0].message.content or ''
        )
        sim = max(
            calculate_similarity(candidate, ex)
            for ex in existing_texts
        )
        if sim < 0.7:
            return candidate, sim, MAX_RETRIES + i
        regen, score = candidate, sim
    return regen, score, MAX_RETRIES * 2


def regenerate_title(content: str) -> str:
    resp = client.ChatCompletion.create(
        model="gpt-4-turbo",
        messages=[
            {"role": "system", "content": "ÎÑàÎäî ÎßàÏºÄÌåÖ ÏΩòÌÖêÏ∏† Ï†ÑÎ¨∏Í∞ÄÏïº. ÏßßÏùÄ Ï†úÎ™©ÏùÑ ÏûëÏÑ±Ìï¥Ï§ò."},
            {"role": "user", "content": content[:1000]},
        ],
        temperature=0.7,
        max_tokens=800,
    )
    return re.sub(
        r'^.*?:\s*', '',
        resp.choices[0].message.content.strip()
    )


def translate_text(text: str, lang: str) -> str:
    resp = client.ChatCompletion.create(
        model=TRANSLATION_MODEL,
        messages=[
            {"role": "system", "content": f"Îã§ÏùåÏùÑ {lang}Î°ú Î≤àÏó≠Ìï¥Ï§ò."},
            {"role": "user", "content": text},
        ],
        temperature=0.5,
        max_tokens=2000,
    )
    return resp.choices[0].message.content.strip()


def now_str() -> str:
    return datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S")

# ‚îÄ‚îÄ ÌîΩÏÇ¨Î≤†Ïù¥ + Pillow ‚Üí Ìè¨Ïä§ÌÑ∞ ÏÉùÏÑ± Î∞è Drive ÏóÖÎ°úÎìú Ìó¨Ìçº Ìï®Ïàò ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
def generate_poster_and_upload(text: str, keywords: List[str]) -> str:
    query = "+".join(keywords)
    resp = requests.get(
        f"https://pixabay.com/api/?key={PIXABAY_API_KEY}&q={query}&image_type=photo&orientation=horizontal"
    )
    data = resp.json()
    if not data.get("hits"):
        return None
    img_bytes = requests.get(
        data["hits"][0]["largeImageURL"]
    ).content
    bg = Image.open(io.BytesIO(img_bytes)).convert("RGBA")
    draw = ImageDraw.Draw(bg)
    font_title = ImageFont.truetype(FONT_PATH, size=60)
    font_body = ImageFont.truetype(FONT_PATH, size=40)
    title, *body_lines = text.split("\n")
    y = 50
    for line in textwrap.wrap(title, width=20):
        draw.text((50, y), line, font=font_title, fill=(255,255,255))
        y += font_title.getsize(line)[1] + 10
    y += 30
    body = " ".join(body_lines)
    for line in textwrap.wrap(body, width=40):
        draw.text((50, y), line, font=font_body, fill=(255,255,255))
        y += font_body.getsize(line)[1] + 5
    bio = io.BytesIO()
    bg.convert("RGB").save(bio, format="JPEG")
    bio.seek(0)
    media = MediaIoBaseUpload(bio, mimetype="image/jpeg")
    fname = f"{keywords[0]}.jpg"
    meta = {"name": fname, "parents": [DRIVE_FOLDER_ID]}
    f = drive_service.files().create(body=meta, media_body=media, fields="id").execute()
    return f"https://drive.google.com/uc?export=download&id={f['id']}"


def process_regeneration():
    logging.basicConfig(level=logging.INFO)
    logging.info("üìå process_regeneration() ÏãúÏûë")

    src_ws = init_worksheet(SOURCE_DB_ID, "ÌôçÎ≥¥ÏãúÌä∏")
    prompt_ws = init_worksheet(SOURCE_DB_ID, "ÌîÑÎ°¨ÌîÑÌä∏ÏãúÌä∏")
    image_ws = init_worksheet(SOURCE_DB_ID, "Ïù¥ÎØ∏ÏßÄ ÏãúÌä∏")
    info_ws = init_worksheet(TARGET_DB_ID, "ÌôçÎ≥¥ÏãúÌä∏")

    src_header = src_ws.row_values(1)
    src_col_map = {name: idx for idx, name in enumerate(src_header)}

    rows = src_ws.get_all_values()[1:]
    today = datetime.datetime.now().date()
    filtered_rows = []
    for r in rows:
        norm = re.sub(r"[\.\s]+", "-", r[1].strip())
        norm = re.sub(r"-+", "-", norm)
        try:
            dl = datetime.datetime.strptime(norm, "%Y-%m-%d").date()
        except ValueError:
            continue
        if dl >= today:
            filtered_rows.append(r)
    if not filtered_rows:
        logging.warning("‚ö†Ô∏è Ïú†Ìö®Ìïú ÎßàÏºÄÌåÖ ÏΩòÌÖêÏ∏†Í∞Ä ÏóÜÏäµÎãàÎã§.")
        return 0

    valid_rows = filtered_rows
    item = random.choice(valid_rows)
    existing_texts = [r[4] for r in valid_rows if r != item]

    prompt_header = prompt_ws.row_values(1)
    col_map = {name: idx for idx, name in enumerate(prompt_header)}
    run_idx = col_map.get("run_count", len(prompt_header))
    all_prompts = prompt_ws.get_all_values()[1:]

    # ‚îÄ‚îÄ ÏàòÏ†ï: "ÌòÑÏû¨ÏÇ¨Ïö©Ïó¨Î∂Ä" 3Í∞ÄÏßÄ ÏòµÏÖò Ï≤òÎ¶¨ ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    valid_status = ("Íµ≠Î¨∏", "Ï†ÑÏ≤¥")
    matching_prompts = [
        cfg for cfg in all_prompts
        if len(cfg) > run_idx
           and cfg[col_map["Ï∂úÏ≤ò"]].strip() == "ÌôçÎ≥¥ÏãúÌä∏"
           and cfg[col_map["ÌòÑÏû¨ÏÇ¨Ïö©Ïó¨Î∂Ä"]].strip() in valid_status  # ‚îÄ‚îÄ ÏàòÏ†ï: Íµ≠Î¨∏/Ï†ÑÏ≤¥Îßå ÌóàÏö©
           and cfg[col_map["Íµ¨Î∂ÑÌÉúÍ∑∏"]].strip()
    ]
    if not matching_prompts:
        raise RuntimeError("‚ùå ÏÇ¨Ïö©Ìï† Ïàò ÏûàÎäî ÌîÑÎ°¨ÌîÑÌä∏Í∞Ä ÏóÜÏäµÎãàÎã§. (Íµ≠Î¨∏/Ï†ÑÏ≤¥Îßå ÌóàÏö©)")
    cfg = matching_prompts[0]
    category = cfg[col_map["Íµ¨Î∂ÑÌÉúÍ∑∏"]].strip()
    usage = cfg[col_map["ÌòÑÏû¨ÏÇ¨Ïö©Ïó¨Î∂Ä"]].strip()  # ‚îÄ‚îÄ Ï∂îÍ∞Ä: ÏÇ¨Ïö© ÏòµÏÖò Ï†ÄÏû•

    prompt_fields = [
        "ÏûëÏÑ±Ïûê Ïó≠Ìï† ÏÑ§Î™Ö", "Ï†ÑÏ≤¥ ÏûëÏÑ± Ï°∞Í±¥", "Í∏Ä Íµ¨ÏÑ±Î∞©Ïãù",
        "ÌïÑÏàò Ìè¨Ìï® Ìï≠Î™©", "ÎßàÎ¨¥Î¶¨ Î¨∏Ïû•", "Ï∂îÍ∞Ä ÏßÄÏãúÏÇ¨Ìï≠"
    ]
    prompt_cfg = [cfg[col_map[f]] for f in prompt_fields]

    orig_title = item[0]
    orig_cont = item[4]
    prev_count = int(cfg[run_idx]) if cfg[run_idx].isdigit() else 0
    interval = int(cfg[col_map["Í∏Ä Í∞ÑÍ≤©"]]) if cfg[col_map["Í∏Ä Í∞ÑÍ≤©"]].isdigit() else 1
    basic_mod = cfg[col_map["Í∏∞Î≥∏ gpt"]].strip() or "gpt-3.5-turbo"
    adv_mod = cfg[col_map["Í≥†Í∏â gpt"]].strip() or basic_mod
    use_model = basic_mod if prev_count < interval else adv_mod
    new_count = prev_count + 1 if prev_count < interval else 0

    content, score, _ = regenerate_unique_post(
        orig_title, orig_cont, existing_texts, prompt_cfg, use_model
    )
    title = regenerate_title(content)

    # ‚îÄ‚îÄ Ïù¥ÎØ∏ÏßÄ ÌÉúÍ∑∏ Îß§Ïπ≠ & ÎåÄÏ≤¥ Î°úÏßÅ ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    image_tag = cfg[col_map["Ïù¥ÎØ∏ÏßÄÌÉúÍ∑∏"]].strip()
    img = ""
    if image_tag:
        img_header = image_ws.row_values(1)
        img_col_map = {name: idx for idx, name in enumerate(img_header)}
        d_idx = img_col_map.get("Ïù¥ÎØ∏ÏßÄÌÉúÍ∑∏")
        c_idx = img_col_map.get("Ïù¥ÎØ∏ÏßÄurl")
        candidates = [
            row[c_idx].strip() for row in image_ws.get_all_values()[1:]
            if len(row) > d_idx and row[d_idx].strip() == image_tag
               and len(row) > c_idx and row[c_idx].strip()
        ]
        if candidates:
            selected = random.choice(candidates)
            prev_img_idx = src_col_map.get("Ïù¥ÎØ∏ÏßÄurl")
            prev_img = item[prev_img_idx].strip() if prev_img_idx is not None else None
            if selected == prev_img:
                kws = re.findall(r"\b[Í∞Ä-Ìû£]{2,}\b", orig_cont)[:3]
                new_url = generate_poster_and_upload(orig_cont, kws)
                img = new_url or selected
            else:
                img = selected

    # ‚îÄ‚îÄ Î≤àÏó≠ Ï≤òÎ¶¨: "Ï†ÑÏ≤¥"Ïùº ÎïåÎßå Î≤àÏó≠, "Íµ≠Î¨∏"Ïùº Í≤ΩÏö∞ Î≤àÏó≠ Í±¥ÎÑàÎúÄ ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    if usage == "Ï†ÑÏ≤¥":
        en = translate_text(content, 'English')
        zh = translate_text(content, 'Chinese')
        ja = translate_text(content, 'Japanese')
    else:
        en = zh = ja = ""

    # ‚îÄ‚îÄ Í≤∞Í≥º Í∏∞Î°ù ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    info_ws.append_row([
        now_str(), title, content, category,
        en, zh, ja, f"{score:.2f}", img
    ])

    # ‚îÄ‚îÄ run_count ÏóÖÎç∞Ïù¥Ìä∏ ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    prompt_ws.update_cell(
        all_prompts.index(cfg) + 2,
        run_idx + 1,
        str(new_count)
    )

    logging.info("üí∞ Ìïú Í±¥Ïùò ÎßàÏºÄÌåÖ ÏΩòÌÖêÏ∏†Í∞Ä ÏÑ±Í≥µÏ†ÅÏúºÎ°ú ÏÉùÏÑ±ÎêòÏóàÏäµÎãàÎã§.")
    return 1


if __name__ == "__main__":
    process_regeneration()
